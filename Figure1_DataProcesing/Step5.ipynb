{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bc71ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pickle, csv, re, os, tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2fed762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 expression matrices found and paths loaded\n"
     ]
    }
   ],
   "source": [
    "# Load static paths\n",
    "root = '/media/sam/New Volume/Xenium_Data'\n",
    "output_root = '/media/sam/Data2/baysor_rbpms_consolidated'\n",
    "os.makedirs(output_root, exist_ok=True) # Make sure output path exists\n",
    "xen_roots = ['output-XETG00230__0018429__Region_1__20240105__233208',\n",
    "             'output-XETG00230__0018432__Region_2__20240105__233208',\n",
    "             'BudoffRun2_Slide 3_4/BudoffRun2_Slide 3_4/output-XETG00230__0018336__Region_1__20240124__002923',\n",
    "             'BudoffRun2_Slide 3_4/BudoffRun2_Slide 3_4/output-XETG00230__0018521__Region_1__20240124__002923',\n",
    "             'BudoffRun3_Slide 5_6/BudoffRun3_Slide 5_6/output-XETG00230__0018624__Region_1__20240127__000149',\n",
    "             'BudoffRun3_Slide 5_6/BudoffRun3_Slide 5_6/output-XETG00230__0022826__Region_1__20240127__000149',\n",
    "             'BudoffRun4_Slide 7_8/BudoffRun4_Slide 7_8/output-XETG00230__0018300__Region_1__20240206__235339',\n",
    "             'BudoffRun4_Slide 7_8/BudoffRun4_Slide 7_8/output-XETG00230__0022825__Region_1__20240206__235339']\n",
    "coords_root = '/media/sam/Data2/xenium_rbpms_coordinates'\n",
    "\n",
    "# Extract slide_ids\n",
    "slides = [xen_root[-33:-28] for xen_root in xen_roots]\n",
    "\n",
    "# Iterate through each slide directory\n",
    "saved_paths = {}\n",
    "for slide_id in slides:\n",
    "    slide_path = os.path.join(output_root, slide_id)\n",
    "    \n",
    "    # Skip if slide directory doesn't exist\n",
    "    if not os.path.exists(slide_path):\n",
    "        continue\n",
    "    \n",
    "    # Get all subdirectories for this slide\n",
    "    exp_mat_paths = [os.path.join(slide_path, d, f'{d[:-8]}_expression_matrix.csv')  for d in os.listdir(slide_path) if os.path.isdir(os.path.join(slide_path, d))]\n",
    "    \n",
    "    for em in exp_mat_paths:\n",
    "        slice_id = em[-27:-22]\n",
    "        if os.path.exists(em):\n",
    "            saved_paths[(slide_id, slice_id)] = em\n",
    "\n",
    "print(f'{len(saved_paths)} expression matrices found and paths loaded')\n",
    "\n",
    "# Load conversion key for puncta to exp mat\n",
    "gene_conversion_path = '/media/sam/Data2/baysor_rbpms_consolidated/Gene_Conversion_Key.csv'\n",
    "gc_df = pd.read_csv(gene_conversion_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df472d29",
   "metadata": {},
   "source": [
    "# CuttleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a2f28e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Dependencies\n",
    "# path = '/home/sam/scRNAseq/Xenium/Network_genes_NoiseInjection.RData'\n",
    "#model_path = \"/home/sam/scRNAseq/Xenium/AlonNN/NoiseInj/model_state_epochs_150_earlyStop_50_l1_0.0001_depth_5_withSkips_seed_18.pt\"\n",
    "\n",
    "\n",
    "path = '/media/sam/Data2/CuttleNet_atlases/OriginalRetina'\n",
    "optimization = 'Noise100'\n",
    "noise_level = ''#'_noise_0.2' #set to '' when no noise\n",
    "model_path = os.path.join(path, f'{optimization}{noise_level}_optimal_model.pt')\n",
    "class_info_path = os.path.join(path, 'optimized_class_info.json')\n",
    "gene_order_path = os.path.join(path, 'input_order.csv')\n",
    "# gene_order_path = os.path.join(path, 'gene_rename_map.csv')\n",
    "mapping_path = os.path.join(path,  optimization, 'mapping.csv')\n",
    "label_encoder_path = os.path.join(path, optimization,  'label_encoder.pkl')\n",
    "clust_id_path = os.path.join(path, optimization, 'clust_ids.csv')\n",
    "\n",
    "# class_info_path = '/media/sam/Data2/baysor_analysis/CuttleNet/class_info.json'\n",
    "# gene_order_path = '/media/sam/Data2/baysor_analysis/CuttleNet/input_order.csv'\n",
    "# mapping_path = '/media/sam/Data2/baysor_analysis/CuttleNet/mapping.csv'\n",
    "# label_encoder_path = '/media/sam/Data2/baysor_analysis/CuttleNet/le.pkl'\n",
    "# clust_id_path = '/media/sam/Data2/baysor_analysis/CuttleNet/clust_ids.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df7637dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_list_from_csv(file_path):\n",
    "    \"\"\"Load a list of string entries from a CSV file.\"\"\"\n",
    "    clust_ids = []\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        # Each row corresponds to one item in the list\n",
    "        for row in reader:\n",
    "            clust_ids.append(row[0])  # Access the first (and only) column in each row\n",
    "    return clust_ids\n",
    "\n",
    "def load_label_encoder(file_path):\n",
    "    \"\"\"Load the LabelEncoder object from a file.\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_mapping_from_csv(file_path):\n",
    "    \"\"\"Load the Cluster-to-Class mapping from a CSV file back into a dictionary.\"\"\"\n",
    "    mapping = {}\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            cluster, class_value = int(row[0]), int(row[1])\n",
    "            mapping[cluster] = class_value\n",
    "    return mapping\n",
    "\n",
    "def convert_to_numpy(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        new_dict = {}\n",
    "        for k, v in obj.items():\n",
    "            # Convert keys back to integers if possible\n",
    "            try:\n",
    "                new_key = int(k)\n",
    "            except ValueError:\n",
    "                new_key = k\n",
    "            new_dict[new_key] = convert_to_numpy(v)\n",
    "        return new_dict\n",
    "    elif isinstance(obj, list):\n",
    "        # Convert lists back to NumPy arrays if they contain numbers\n",
    "        if all(isinstance(i, (int, float)) for i in obj):\n",
    "            return np.array(obj)\n",
    "        else:\n",
    "            return [convert_to_numpy(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "def load_class_info(class_info_path):    \n",
    "    # Load the JSON file\n",
    "    with open(class_info_path, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        new_dict = {}\n",
    "        for k, v in obj.items():\n",
    "            # Convert keys back to integers if possible\n",
    "            try:\n",
    "                new_key = int(k)\n",
    "            except ValueError:\n",
    "                new_key = k\n",
    "            new_dict[new_key] = convert_to_numpy(v)\n",
    "        return new_dict\n",
    "    elif isinstance(obj, list):\n",
    "        # Convert lists back to NumPy arrays if they contain numbers\n",
    "        if all(isinstance(i, (int, float)) for i in obj):\n",
    "            return np.array(obj)\n",
    "        else:\n",
    "            return [convert_to_numpy(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "    \n",
    "class TentacleNet(nn.Module):\n",
    "    def __init__(self, input_size, num_subclasses, num_hidden, skip = False):\n",
    "        super(TentacleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2*num_subclasses)\n",
    "        self.num_hidden = num_hidden\n",
    "        self.skip = skip+0\n",
    "        if self.num_hidden > 0:\n",
    "            self.hidden = nn.ModuleList([nn.Linear(2*num_subclasses, 2*num_subclasses) for _ in range(num_hidden)])\n",
    "        self.fc2 = nn.Linear(2*num_subclasses, num_subclasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        if self.num_hidden > 0:\n",
    "            x_skip = x*self.skip  # Save output of fc1 for skip connection\n",
    "            for hidden_layer in self.hidden:\n",
    "                x = nn.functional.relu(hidden_layer(x))\n",
    "            x = x + x_skip  # Add skip connection before final activation\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class CuttleNet(nn.Module):\n",
    "    def __init__(self, class_info, mapping):\n",
    "        super(CuttleNet, self).__init__()\n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.n = len(mapping) # Number of subclasses\n",
    "        self.class_info = class_info\n",
    "\n",
    "        # Class Classifier\n",
    "        self.class_fc1 = nn.Linear(len(class_info['Genes']), 2*class_info['num_classes'])\n",
    "        self.class_fc2 = nn.Linear(2*class_info['num_classes'], class_info['num_classes'])\n",
    "\n",
    "        # Subclass Classifiers\n",
    "        self.subclass_nets = nn.ModuleDict({\n",
    "            str(class_id): TentacleNet(input_size=len(subclass_info['Genes']) + class_info['num_classes'], \n",
    "                                       num_subclasses=subclass_info['num_subclasses'],\n",
    "                                      num_hidden = subclass_info['num_hidden'],\n",
    "                                      skip = subclass_info['skip'])\n",
    "            for class_id, subclass_info in class_info.items()\n",
    "            if isinstance(class_id, int)\n",
    "        })\n",
    "        \n",
    "        # Calculate the number of subclasses for each class\n",
    "        self.num_subclasses_per_class = self.calculate_subclasses_per_class(mapping)\n",
    "        \n",
    "    def get_subclass_range_for_class(self, class_id):\n",
    "        start_index = sum(self.num_subclasses_per_class[cid] for cid in range(class_id))\n",
    "        end_index = start_index + self.num_subclasses_per_class[class_id]\n",
    "        return slice(start_index, end_index)\n",
    "    \n",
    "    def calculate_subclasses_per_class(self, mapping):\n",
    "        \"\"\"\n",
    "        Calculate the number of subclasses for each class using the mapping.\n",
    "        \"\"\"\n",
    "        num_subclasses_per_class = {class_id: 0 for class_id in range(self.class_info['num_classes'])}\n",
    "        for subclass_id in mapping.keys():\n",
    "            class_id = mapping[subclass_id]\n",
    "            num_subclasses_per_class[class_id] += 1\n",
    "        return num_subclasses_per_class\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Class classification\n",
    "        class_genes = x[:, self.class_info['Genes']]\n",
    "        class_x = nn.functional.relu(self.class_fc1(class_genes))\n",
    "        class_output = nn.functional.log_softmax(self.class_fc2(class_x), dim=1)\n",
    "\n",
    "        # Initialize an output tensor for all subclasses\n",
    "        all_subclass_output = torch.zeros(x.size(0), self.n, device=self.device)  # Assuming 130 total subclasses\n",
    "\n",
    "        # Populate the output tensor\n",
    "        for class_id, subclass_info in self.class_info.items():\n",
    "            if isinstance(class_id, int):\n",
    "                subclass_genes = x[:, subclass_info['Genes']]\n",
    "                subclass_input = torch.cat((subclass_genes, class_output), dim=1)\n",
    "\n",
    "                # Convert class_id to string\n",
    "                class_id_str = str(class_id)\n",
    "                subclass_output = self.subclass_nets[class_id_str](subclass_input)\n",
    "\n",
    "                # Get the range for this class's subclasses\n",
    "                subclass_range = self.get_subclass_range_for_class(class_id)\n",
    "\n",
    "                # Multiply subclass predictions by the class prediction probability\n",
    "                all_subclass_output[:, subclass_range] = subclass_output * class_output[:, class_id].unsqueeze(1)\n",
    "\n",
    "        return all_subclass_output\n",
    "\n",
    "def exp_mat_converter(df, gc_df):\n",
    "    '''This funtion converts a puncta dataframe into an expression matrix equivelent using empirical means and factors'''\n",
    "    out = df.copy()\n",
    "    for i, (gene, F, mu) in gc_df.iterrows():\n",
    "        out[gene] = F*(out[gene] - mu) # Apply conversion\n",
    "        out[gene] = out[gene] * (out[gene]>10e-15)+0 # Set negative values to 0\n",
    "        out[gene] = np.log(out[gene], where=0<out[gene]) # Log transform ignoring 0s\n",
    "        out[gene] = out[gene] * (out[gene]>10e-15)+0 # Remove rounding errors\n",
    "    return out\n",
    "\n",
    "def median_norm_converter(df, factor = 1):\n",
    "    \"\"\"\n",
    "    Normalize gene expression counts and apply log transformation as per Karthik 2016.\n",
    "    Each cell (row) is normalized to sum to the median number of transcripts per cell,\n",
    "    followed by log transformation.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe where columns are genes and rows are cells\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Normalized and log-transformed expression matrix\n",
    "    \"\"\"    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    out = df.copy()\n",
    "    \n",
    "    # Calculate the sum of transcripts per cell (row sums)\n",
    "    transcripts_per_cell = out.sum(axis=1)\n",
    "    \n",
    "    # Calculate the median number of transcripts per cell\n",
    "    median_transcripts = transcripts_per_cell.median()\n",
    "    \n",
    "    # Calculate scaling factor for each cell (row)\n",
    "    scaling_factors = median_transcripts / transcripts_per_cell\n",
    "    \n",
    "    # Apply scaling factors to each row\n",
    "    # Broadcasting the scaling factors to multiply with each row\n",
    "    out = out.multiply(scaling_factors, axis=0)\n",
    "    \n",
    "    # Apply log transformation: ln(M_ij + 1)\n",
    "    out = factor*np.log(out + 1)\n",
    "    \n",
    "    out = out*factor\n",
    "    \n",
    "    return out\n",
    "\n",
    "def CuttleNet_Inference(exp_matrix_df, clust_ids, gene_order, model, \n",
    "                        median_norm_ln = True,factor = 1,\n",
    "                        thousandth = False, convert = False, chunk_size=10000):\n",
    "    \n",
    "    if convert:\n",
    "        # Convert df from puncta to expression\n",
    "        exp_matrix_df = exp_mat_converter(exp_matrix_df, gc_df)\n",
    "    \n",
    "    \n",
    "    # Reorder columns to match the correct order of genes (correct_order)\n",
    "    data = exp_matrix_df[gene_order]  # Only keep the columns in correct_order\n",
    "    \n",
    "    if median_norm_ln:\n",
    "        # convert to a median normalized log transformed expression matrix as per Karthik 2016\n",
    "        data = median_norm_converter(data, factor)\n",
    "    \n",
    "    if thousandth:\n",
    "        # Normalize each column by its maximum value, multiply by 1000, round, and divide by 1000\n",
    "        data = data.apply(lambda x: round(1000 * x / x.max()) / 1000)\n",
    "    \n",
    "    # Store cell IDs\n",
    "    cell_ids = list(exp_matrix_df['cell'])\n",
    "\n",
    "    # Convert to torch tensor and store on GPU\n",
    "    expMatrix = data.to_numpy()\n",
    "    expMatrix = torch.tensor(expMatrix, dtype=torch.float32)\n",
    "\n",
    "    print(f'Data loaded containing {len(cell_ids)} cells')\n",
    "\n",
    "    # Calculate the number of chunks needed\n",
    "    n_chunks = int(np.ceil(expMatrix.size(0) / chunk_size))\n",
    "\n",
    "    # Placeholder to collect the output\n",
    "    results = []\n",
    "\n",
    "    print('Performing Inference')\n",
    "    # Process each chunk\n",
    "    for i in tqdm.tqdm(range(n_chunks)):\n",
    "        # Calculate the start and end indices of the current chunk\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, expMatrix.size(0))\n",
    "\n",
    "        # Extract the chunk\n",
    "        chunk = expMatrix[start_idx:end_idx]\n",
    "\n",
    "        # Move the chunk to GPU\n",
    "        chunk = chunk.to('cuda')\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():  # Ensure gradients are not computed to save memory\n",
    "            chunk_output = model(chunk)\n",
    "\n",
    "        # Move the results back to CPU and store them\n",
    "        chunk_output = chunk_output.cpu()\n",
    "        results.append(chunk_output)\n",
    "\n",
    "    # Concatenate the results into a single tensor\n",
    "    final_results = torch.cat(results, dim=0)\n",
    "\n",
    "    # Create a DataFrame with the inference results\n",
    "    final_df = pd.DataFrame(final_results.numpy(), columns=clust_ids)\n",
    "    \n",
    "    # Add Prediction column\n",
    "    final_df['Prediction'] = final_df.idxmax(axis=1)\n",
    "    \n",
    "    # Add cell_ids column\n",
    "    final_df['cell'] = cell_ids\n",
    "\n",
    "    print('Inference and dataframe merging complete.')\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d25f013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/sam/Data2/CuttleNet_atlases/OriginalRetina/Noise100_optimal_model.pt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf572887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found gene rename map at /media/sam/Data2/CuttleNet_atlases/OriginalRetina/gene_rename_map.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CuttleNet(\n",
       "  (class_fc1): Linear(in_features=242, out_features=12, bias=True)\n",
       "  (class_fc2): Linear(in_features=12, out_features=6, bias=True)\n",
       "  (subclass_nets): ModuleDict(\n",
       "    (0): TentacleNet(\n",
       "      (fc1): Linear(in_features=299, out_features=90, bias=True)\n",
       "      (hidden): ModuleList(\n",
       "        (0): Linear(in_features=90, out_features=90, bias=True)\n",
       "      )\n",
       "      (fc2): Linear(in_features=90, out_features=45, bias=True)\n",
       "    )\n",
       "    (1): TentacleNet(\n",
       "      (fc1): Linear(in_features=305, out_features=126, bias=True)\n",
       "      (fc2): Linear(in_features=126, out_features=63, bias=True)\n",
       "    )\n",
       "    (2): TentacleNet(\n",
       "      (fc1): Linear(in_features=252, out_features=4, bias=True)\n",
       "      (fc2): Linear(in_features=4, out_features=2, bias=True)\n",
       "    )\n",
       "    (3): TentacleNet(\n",
       "      (fc1): Linear(in_features=252, out_features=2, bias=True)\n",
       "      (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
       "    )\n",
       "    (4): TentacleNet(\n",
       "      (fc1): Linear(in_features=252, out_features=28, bias=True)\n",
       "      (hidden): ModuleList(\n",
       "        (0-4): 5 x Linear(in_features=28, out_features=28, bias=True)\n",
       "      )\n",
       "      (fc2): Linear(in_features=28, out_features=14, bias=True)\n",
       "    )\n",
       "    (5): TentacleNet(\n",
       "      (fc1): Linear(in_features=248, out_features=2, bias=True)\n",
       "      (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model structuring information\n",
    "class_info = load_class_info(class_info_path)\n",
    "gene_order = np.loadtxt(gene_order_path, delimiter=\",\", dtype=str)\n",
    "rename_map_path = os.path.join(path, 'gene_rename_map.csv')\n",
    "if os.path.exists(rename_map_path):\n",
    "    print(f\"\\nFound gene rename map at {rename_map_path}\")\n",
    "    # Load rename map\n",
    "    rename_df = pd.read_csv(rename_map_path)\n",
    "    rename_map = dict(zip(rename_df.iloc[:, 0], rename_df.iloc[:, 1]))\n",
    "    gene_order = np.array([rename_map.get(str(gene), str(gene)) for gene in gene_order])\n",
    "\n",
    "\n",
    "mapping = load_mapping_from_csv(mapping_path)\n",
    "le = load_label_encoder(label_encoder_path)\n",
    "clust_ids = load_list_from_csv(clust_id_path)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CuttleNet(class_info=class_info, mapping=mapping)\n",
    "\n",
    "# Load the model state\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "# Move the model to the appropriate device and set it to evaluation mode\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "229f4332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded containing 4247 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 390.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18429 R1.01 predictions completed and saved\n",
      "Data loaded containing 2363 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 453.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18429 R1.02 predictions completed and saved\n",
      "Data loaded containing 16307 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 190.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18429 R1.03 predictions completed and saved\n",
      "Data loaded containing 14873 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 210.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18429 R1.04 predictions completed and saved\n",
      "Data loaded containing 22972 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 229.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n",
      "18429 R1.05 predictions completed and saved\n",
      "Data loaded containing 42122 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 212.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18429 R1.06 predictions completed and saved\n",
      "Data loaded containing 45892 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 199.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18429 R1.07 predictions completed and saved\n",
      "Data loaded containing 70999 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 184.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18429 R1.08 predictions completed and saved\n",
      "Data loaded containing 35095 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 183.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18432 R1.09 predictions completed and saved\n",
      "Data loaded containing 37693 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 173.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18432 R1.10 predictions completed and saved\n",
      "Data loaded containing 19904 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 164.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18432 R1.11 predictions completed and saved\n",
      "Data loaded containing 3519 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 430.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18432 R1.12 predictions completed and saved\n",
      "Data loaded containing 814 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 600.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18336 R2.01 predictions completed and saved\n",
      "Data loaded containing 841 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 372.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18336 R2.02 predictions completed and saved\n",
      "Data loaded containing 996 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 261.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18336 R2.03 predictions completed and saved\n",
      "Data loaded containing 2664 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 360.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18336 R2.04 predictions completed and saved\n",
      "Data loaded containing 7287 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 254.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18336 R2.05 predictions completed and saved\n",
      "Data loaded containing 15509 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 184.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18336 R2.06 predictions completed and saved\n",
      "Data loaded containing 44996 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 199.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18336 R2.07 predictions completed and saved\n",
      "Data loaded containing 94809 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 197.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18521 R2.09 predictions completed and saved\n",
      "Data loaded containing 85856 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 9/9 [00:00<00:00, 187.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18521 R2.11 predictions completed and saved\n",
      "Data loaded containing 62910 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 7/7 [00:00<00:00, 157.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18521 R2.12 predictions completed and saved\n",
      "Data loaded containing 51346 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 6/6 [00:00<00:00, 208.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18521 R2.13 predictions completed and saved\n",
      "Data loaded containing 24588 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 216.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18521 R2.14 predictions completed and saved\n",
      "Data loaded containing 22621 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 214.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18521 R2.15 predictions completed and saved\n",
      "Data loaded containing 15840 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 202.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18521 R2.16 predictions completed and saved\n",
      "Data loaded containing 1051 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 477.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18521 R2.17 predictions completed and saved\n",
      "Data loaded containing 1469 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 531.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18624 R3.03 predictions completed and saved\n",
      "Data loaded containing 2763 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 503.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18624 R3.04 predictions completed and saved\n",
      "Data loaded containing 5242 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 262.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18624 R3.05 predictions completed and saved\n",
      "Data loaded containing 6130 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 289.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18624 R3.06 predictions completed and saved\n",
      "Data loaded containing 16417 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 179.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18624 R3.07 predictions completed and saved\n",
      "Data loaded containing 59737 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 6/6 [00:00<00:00, 178.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18624 R3.08 predictions completed and saved\n",
      "Data loaded containing 3787 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 402.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22826 R4.05 predictions completed and saved\n",
      "Data loaded containing 5981 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 272.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22826 R4.06 predictions completed and saved\n",
      "Data loaded containing 29458 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 174.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22826 R4.07 predictions completed and saved\n",
      "Data loaded containing 48774 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 154.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22826 R4.08 predictions completed and saved\n",
      "Data loaded containing 65370 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 7/7 [00:00<00:00, 168.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22826 R4.09 predictions completed and saved\n",
      "Data loaded containing 86610 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 9/9 [00:00<00:00, 192.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22826 R4.10 predictions completed and saved\n",
      "Data loaded containing 68890 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 7/7 [00:00<00:00, 188.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22826 R4.11 predictions completed and saved\n",
      "Data loaded containing 57786 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 6/6 [00:00<00:00, 183.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22826 R4.12 predictions completed and saved\n",
      "Data loaded containing 57700 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 6/6 [00:00<00:00, 190.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18300 R4.13 predictions completed and saved\n",
      "Data loaded containing 8933 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 217.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18300 R4.14 predictions completed and saved\n",
      "Data loaded containing 487 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 233.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18300 R5.01 predictions completed and saved\n",
      "Data loaded containing 1374 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 437.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18300 R5.02 predictions completed and saved\n",
      "Data loaded containing 2684 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 526.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18300 R5.03 predictions completed and saved\n",
      "Data loaded containing 3277 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 443.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18300 R5.04 predictions completed and saved\n",
      "Data loaded containing 7730 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 235.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18300 R5.06 predictions completed and saved\n",
      "Data loaded containing 15377 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 206.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18300 R5.07 predictions completed and saved\n",
      "Data loaded containing 27594 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 152.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18300 R5.16 predictions completed and saved\n",
      "Data loaded containing 8969 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 208.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18300 R5.17 predictions completed and saved\n",
      "Data loaded containing 454 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 284.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22825 R3.09 predictions completed and saved\n",
      "Data loaded containing 76621 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 178.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22825 R3.10 predictions completed and saved\n",
      "Data loaded containing 38250 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 180.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22825 R3.13 predictions completed and saved\n",
      "Data loaded containing 8880 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 215.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22825 R3.14 predictions completed and saved\n",
      "Data loaded containing 157 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 682.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22825 R4.01 predictions completed and saved\n",
      "Data loaded containing 339 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 276.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22825 R4.02 predictions completed and saved\n",
      "Data loaded containing 344 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 251.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22825 R4.03 predictions completed and saved\n",
      "Data loaded containing 1950 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 520.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22825 R4.04 predictions completed and saved\n",
      "Data loaded containing 44437 cells\n",
      "Performing Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 184.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference and dataframe merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22825 R5.15 predictions completed and saved\n"
     ]
    }
   ],
   "source": [
    "for slide, rslice in saved_paths:\n",
    "    # load current paths\n",
    "    path = saved_paths[(slide,rslice)]\n",
    "    preds_path = f'{path[:-21]}ClassProbabilities.csv'\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    \n",
    "    # Perform inference\n",
    "    preds_df = CuttleNet_Inference(df, le.classes_, gene_order, model, \n",
    "                                   factor = 1,\n",
    "                                   convert=False, thousandth=False)\n",
    "    # Store most likely class prediction\n",
    "    vals = preds_df[['Prediction', 'cell']]\n",
    "    # Merge the prediction with the existing csv\n",
    "    df = df.drop(columns=['Prediction'], errors='ignore')  # errors='ignore' prevents error if column doesn't exist\n",
    "    df = pd.merge(vals, df, on = 'cell')\n",
    "    # Save outputs\n",
    "    df.to_csv(path, index=False)\n",
    "    preds_df.to_csv(preds_path, index=False)\n",
    "    print(f'{slide} {rslice} predictions completed and saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d4b1476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>cell</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>volume</th>\n",
       "      <th>x_range</th>\n",
       "      <th>y_range</th>\n",
       "      <th>z_range</th>\n",
       "      <th>rect_vol</th>\n",
       "      <th>...</th>\n",
       "      <th>Cacna1a</th>\n",
       "      <th>Kcnj9</th>\n",
       "      <th>Kcnab2</th>\n",
       "      <th>Glrb</th>\n",
       "      <th>Rbpms</th>\n",
       "      <th>Vamp1</th>\n",
       "      <th>Cspg4</th>\n",
       "      <th>Kcnq1ot1</th>\n",
       "      <th>Cdh5</th>\n",
       "      <th>Foxp1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17_Tbr1_S1</td>\n",
       "      <td>CR21cd95893-1000</td>\n",
       "      <td>1239.058313</td>\n",
       "      <td>21137.604547</td>\n",
       "      <td>18.597097</td>\n",
       "      <td>179.210090</td>\n",
       "      <td>11.988281</td>\n",
       "      <td>12.994141</td>\n",
       "      <td>6.070717</td>\n",
       "      <td>945.680564</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AC_56</td>\n",
       "      <td>CR21cd95893-100004</td>\n",
       "      <td>4103.869783</td>\n",
       "      <td>21779.614000</td>\n",
       "      <td>20.307500</td>\n",
       "      <td>223.622030</td>\n",
       "      <td>13.766602</td>\n",
       "      <td>13.816406</td>\n",
       "      <td>9.777697</td>\n",
       "      <td>1859.766391</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AC_54</td>\n",
       "      <td>CR21cd95893-100005</td>\n",
       "      <td>4406.401708</td>\n",
       "      <td>21484.028274</td>\n",
       "      <td>17.382168</td>\n",
       "      <td>19.588107</td>\n",
       "      <td>8.679199</td>\n",
       "      <td>8.476562</td>\n",
       "      <td>4.995476</td>\n",
       "      <td>367.516026</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AC_54</td>\n",
       "      <td>CR21cd95893-100006</td>\n",
       "      <td>4351.024194</td>\n",
       "      <td>21531.617830</td>\n",
       "      <td>22.265369</td>\n",
       "      <td>38.252798</td>\n",
       "      <td>9.301758</td>\n",
       "      <td>10.039062</td>\n",
       "      <td>9.844788</td>\n",
       "      <td>919.315402</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AC_15</td>\n",
       "      <td>CR21cd95893-100007</td>\n",
       "      <td>3966.576506</td>\n",
       "      <td>21917.438495</td>\n",
       "      <td>19.387869</td>\n",
       "      <td>18.790309</td>\n",
       "      <td>9.271484</td>\n",
       "      <td>7.437500</td>\n",
       "      <td>6.083277</td>\n",
       "      <td>419.482477</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44432</th>\n",
       "      <td>AC_54</td>\n",
       "      <td>CR21cd95893-99992</td>\n",
       "      <td>4324.209708</td>\n",
       "      <td>21571.828935</td>\n",
       "      <td>17.243437</td>\n",
       "      <td>18.364607</td>\n",
       "      <td>9.872559</td>\n",
       "      <td>10.328125</td>\n",
       "      <td>6.816118</td>\n",
       "      <td>695.005627</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44433</th>\n",
       "      <td>AC_56</td>\n",
       "      <td>CR21cd95893-99994</td>\n",
       "      <td>4154.189171</td>\n",
       "      <td>21725.603720</td>\n",
       "      <td>23.441039</td>\n",
       "      <td>86.396009</td>\n",
       "      <td>12.251465</td>\n",
       "      <td>11.593750</td>\n",
       "      <td>8.637831</td>\n",
       "      <td>1226.921110</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44434</th>\n",
       "      <td>AC_56</td>\n",
       "      <td>CR21cd95893-99997</td>\n",
       "      <td>4415.319927</td>\n",
       "      <td>21470.412829</td>\n",
       "      <td>23.037162</td>\n",
       "      <td>32.897163</td>\n",
       "      <td>7.761719</td>\n",
       "      <td>12.177734</td>\n",
       "      <td>10.558607</td>\n",
       "      <td>998.001119</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44435</th>\n",
       "      <td>AC_56</td>\n",
       "      <td>CR21cd95893-99998</td>\n",
       "      <td>4263.987232</td>\n",
       "      <td>21612.337212</td>\n",
       "      <td>22.598032</td>\n",
       "      <td>99.122300</td>\n",
       "      <td>10.079102</td>\n",
       "      <td>8.695312</td>\n",
       "      <td>12.200575</td>\n",
       "      <td>1069.269824</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44436</th>\n",
       "      <td>AC_8</td>\n",
       "      <td>CR21cd95893-99999</td>\n",
       "      <td>4316.448809</td>\n",
       "      <td>21573.633511</td>\n",
       "      <td>18.524120</td>\n",
       "      <td>11.161308</td>\n",
       "      <td>9.681152</td>\n",
       "      <td>12.058594</td>\n",
       "      <td>6.442904</td>\n",
       "      <td>752.151647</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44437 rows × 314 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Prediction                cell            x             y          z  \\\n",
       "0      17_Tbr1_S1    CR21cd95893-1000  1239.058313  21137.604547  18.597097   \n",
       "1           AC_56  CR21cd95893-100004  4103.869783  21779.614000  20.307500   \n",
       "2           AC_54  CR21cd95893-100005  4406.401708  21484.028274  17.382168   \n",
       "3           AC_54  CR21cd95893-100006  4351.024194  21531.617830  22.265369   \n",
       "4           AC_15  CR21cd95893-100007  3966.576506  21917.438495  19.387869   \n",
       "...           ...                 ...          ...           ...        ...   \n",
       "44432       AC_54   CR21cd95893-99992  4324.209708  21571.828935  17.243437   \n",
       "44433       AC_56   CR21cd95893-99994  4154.189171  21725.603720  23.441039   \n",
       "44434       AC_56   CR21cd95893-99997  4415.319927  21470.412829  23.037162   \n",
       "44435       AC_56   CR21cd95893-99998  4263.987232  21612.337212  22.598032   \n",
       "44436        AC_8   CR21cd95893-99999  4316.448809  21573.633511  18.524120   \n",
       "\n",
       "           volume    x_range    y_range    z_range     rect_vol  ...  Cacna1a  \\\n",
       "0      179.210090  11.988281  12.994141   6.070717   945.680564  ...        2   \n",
       "1      223.622030  13.766602  13.816406   9.777697  1859.766391  ...        0   \n",
       "2       19.588107   8.679199   8.476562   4.995476   367.516026  ...        0   \n",
       "3       38.252798   9.301758  10.039062   9.844788   919.315402  ...        0   \n",
       "4       18.790309   9.271484   7.437500   6.083277   419.482477  ...        0   \n",
       "...           ...        ...        ...        ...          ...  ...      ...   \n",
       "44432   18.364607   9.872559  10.328125   6.816118   695.005627  ...        1   \n",
       "44433   86.396009  12.251465  11.593750   8.637831  1226.921110  ...        0   \n",
       "44434   32.897163   7.761719  12.177734  10.558607   998.001119  ...        0   \n",
       "44435   99.122300  10.079102   8.695312  12.200575  1069.269824  ...        0   \n",
       "44436   11.161308   9.681152  12.058594   6.442904   752.151647  ...        1   \n",
       "\n",
       "       Kcnj9  Kcnab2  Glrb  Rbpms  Vamp1  Cspg4  Kcnq1ot1  Cdh5  Foxp1  \n",
       "0          2      11     3     10      5      0         2     0      0  \n",
       "1          6       9     1      0      1      0         4     0      0  \n",
       "2          0       1     1      0      1      0         0     0      0  \n",
       "3          4       0     0      0      1      0         0     0      1  \n",
       "4          0       2     1      0      0      0         0     0      0  \n",
       "...      ...     ...   ...    ...    ...    ...       ...   ...    ...  \n",
       "44432      1       1     0      0      0      0         1     1      0  \n",
       "44433      2       4     0      0      0      0         0     0      0  \n",
       "44434      2       4     0      0      1      0         0     0      0  \n",
       "44435      9       2     2      0      0      0         2     0      0  \n",
       "44436      3      11     1      0      0      0         1     1      0  \n",
       "\n",
       "[44437 rows x 314 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c05953c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     original \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([original, data_0])\n",
      "File \u001b[0;32m~/anaconda3/envs/MLM/lib/python3.9/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLM/lib/python3.9/site-packages/pandas/core/reshape/concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m~/anaconda3/envs/MLM/lib/python3.9/site-packages/pandas/core/internals/concat.py:146\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    140\u001b[0m             \u001b[38;5;28mall\u001b[39m(_is_homogeneous_mgr(mgr, first_dtype) \u001b[38;5;28;01mfor\u001b[39;00m mgr, _ \u001b[38;5;129;01min\u001b[39;00m mgrs_indexers)\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m         ):\n\u001b[1;32m    143\u001b[0m             \u001b[38;5;66;03m# Fastpath!\u001b[39;00m\n\u001b[1;32m    144\u001b[0m             \u001b[38;5;66;03m# Length restriction is just to avoid having to worry about 'copy'\u001b[39;00m\n\u001b[1;32m    145\u001b[0m             shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m axes)\n\u001b[0;32m--> 146\u001b[0m             nb \u001b[38;5;241m=\u001b[39m \u001b[43m_concat_homogeneous_fastpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m BlockManager((nb,), axes)\n\u001b[1;32m    149\u001b[0m mgrs \u001b[38;5;241m=\u001b[39m _maybe_reindex_columns_na_proxy(axes, mgrs_indexers, needs_copy)\n",
      "File \u001b[0;32m~/anaconda3/envs/MLM/lib/python3.9/site-packages/pandas/core/internals/concat.py:262\u001b[0m, in \u001b[0;36m_concat_homogeneous_fastpath\u001b[0;34m(mgrs_indexers, shape, first_dtype)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m indexers \u001b[38;5;28;01mfor\u001b[39;00m _, indexers \u001b[38;5;129;01min\u001b[39;00m mgrs_indexers):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# https://github.com/pandas-dev/pandas/pull/52685#issuecomment-1523287739\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [mgr\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mT \u001b[38;5;28;01mfor\u001b[39;00m mgr, _ \u001b[38;5;129;01min\u001b[39;00m mgrs_indexers]\n\u001b[0;32m--> 262\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    263\u001b[0m     bp \u001b[38;5;241m=\u001b[39m libinternals\u001b[38;5;241m.\u001b[39mBlockPlacement(\u001b[38;5;28mslice\u001b[39m(shape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m    264\u001b[0m     nb \u001b[38;5;241m=\u001b[39m new_block_2d(arr, bp)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bin_path = '/media/sam/Data2/baysor_rbpms_consolidated/counts_converted.csv'\n",
    "\n",
    "\n",
    "first = True\n",
    "for slide, rslice in saved_paths:\n",
    "    # load current paths\n",
    "    path = saved_paths[(slide,rslice)]\n",
    "    preds_path = f'{path[:-21]}ClassProbabilities.csv'\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Reorder columns to match the correct order of genes (correct_order)\n",
    "    data_0 = df[gene_order]  # Only keep the columns in correct_order\n",
    "    \n",
    "    # Perform transformation\n",
    "    data_1 = median_norm_converter(data_0)\n",
    "    \n",
    "    \n",
    "    if first:\n",
    "        transformed = data_1.copy()\n",
    "        original = data_0.copy() \n",
    "        first = False\n",
    "    else:\n",
    "        transformed = pd.concat([transformed, data_1])\n",
    "        original = pd.concat([original, data_0])\n",
    "\n",
    "\n",
    "# # First let's get meaningful ranges for each column\n",
    "# all_counts = {}\n",
    "# bin_edges = np.linspace(0, 5, 251)  # 50 bins from 0 to 1 since most values appear to be in this range\n",
    "\n",
    "# # Calculate histogram for each column\n",
    "# for column in hist_df.columns:\n",
    "#     # Get the data excluding zeros and huge outliers\n",
    "#     col_data = hist_df[column].values\n",
    "#     col_data = col_data[col_data > 0]  # Exclude zeros since they dominate\n",
    "#     col_data = col_data[col_data < 1]  # Focus on the main distribution\n",
    "    \n",
    "#     # Calculate histogram\n",
    "#     counts, _ = np.histogram(col_data, bins=bin_edges)\n",
    "#     all_counts[f'{column}_counts'] = counts\n",
    "\n",
    "# # Create DataFrame\n",
    "# binned_count_df = pd.DataFrame(all_counts)\n",
    "# binned_count_df.insert(0, 'bin_edges', bin_edges[:-1])\n",
    "\n",
    "# # Print sample to verify\n",
    "# print(\"\\nOutput shape:\", binned_count_df.shape)\n",
    "# print(\"Sample of binned counts for first few columns:\")\n",
    "# print(binned_count_df.iloc[0:5, 0:4])\n",
    "    \n",
    "# hist_df.to_csv(bin_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create gc_df with grid of F and mu values\n",
    "# genes = [f'gene_{i}_{j}' for i in [1,2,3] for j in [2,3,4]]  # This creates 9 genes\n",
    "# F_values = np.repeat([1, 2, 3], 3)\n",
    "# mu_values = np.tile([2, 3, 4], 3)\n",
    "\n",
    "# # Create gc_df without setting index\n",
    "# gc_df = pd.DataFrame({\n",
    "#     'gene': genes,\n",
    "#     'F': F_values,\n",
    "#     'mu': mu_values\n",
    "# })\n",
    "\n",
    "# # Create dummy dataset with values 1-9\n",
    "# n_samples = 5  # 5 rows of test data\n",
    "# test_data = np.array([range(1, 11)] * len(genes)).T  # Transpose to get 10 rows x 9 columns\n",
    "# df = pd.DataFrame(test_data, columns=genes)\n",
    "\n",
    "# transformed = exp_mat_converter(df, gc_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1efefcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed.to_csv(f'/media/sam/Data2/baysor_rbpms_consolidated/dummydataconverted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57009e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, gene in enumerate(gene_order):\n",
    "    test = pd.DataFrame({'transformed' : transformed[gene],\n",
    "                         'original' : original[gene],\n",
    "                         'gene' : gene})\n",
    "    \n",
    "    test.to_csv(f'/media/sam/Data2/baysor_rbpms_consolidated/{gene}_converted.csv')\n",
    "    \n",
    "    if i > 10:\n",
    "        break\n",
    "display(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b259781",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_median = median_norm_converter(original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72676b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_gene_histogram(data, gene_index, gene_order):\n",
    "    \"\"\"\n",
    "    Creates a histogram for a specific gene's expression values from transformed data.\n",
    "    \n",
    "    Parameters:\n",
    "    transformed_data: pandas DataFrame containing the transformed gene expression data\n",
    "    gene_index: integer index of the gene to plot (0-299)\n",
    "    gene_order: list of gene names in order\n",
    "    \n",
    "    Returns:\n",
    "    None (displays plot)\n",
    "    \"\"\"\n",
    "    if gene_index < 0 or gene_index >= len(gene_order):\n",
    "        raise ValueError(f\"Gene index must be between 0 and {len(gene_order)-1}\")\n",
    "    \n",
    "    gene_name = gene_order[gene_index]\n",
    "    gene_data = data[gene_name]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(gene_data, bins=50, edgecolor='black')\n",
    "    plt.title(f'Distribution of Expression Values for {gene_name}')\n",
    "    plt.xlabel('Expression Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Add summary statistics\n",
    "    mean_val = np.mean(gene_data)\n",
    "    median_val = np.median(gene_data)\n",
    "    \n",
    "    plt.axvline(mean_val, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean_val:.2f}')\n",
    "    plt.axvline(median_val, color='green', linestyle='dashed', linewidth=1, label=f'Median: {median_val:.2f}')\n",
    "    \n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_gene_histogram(all_data_median, 0, gene_order)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
